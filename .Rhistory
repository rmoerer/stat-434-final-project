add_recipe(tree_rec)
tree_grid_search <- tune_grid(
tree_tune_wflow,
resamples = train_cv,
grid = tree_grid
)
tree_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean))
tree_best_acc <- select_best(tree_grid_search, "accuracy")
tree_spec <- finalize_model(
tree_tune_spec,
tree_best_acc
)
tree_wflow <- workflow() %>%
add_model(tree_spec) %>%
add_recipe(tree_rec)
tree_final_fit <- tree_wflow %>%
fit(train)
library(vip)
tree_final_fit %>%
pull_workflow_fit() %>%
vip(geom="col") +
theme_minimal()
rf_grid <- grid_regular(
min_n(),
levels = 10
)
rf_tune_spec <- rand_forest(min_n = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_rec <- tree_rec
rf_tune_wflow <- workflow() %>%
add_model(rf_tune_spec) %>%
add_recipe(rf_rec)
rf_grid_search <- tune_grid(
rf_tune_wflow,
resamples = train_cv,
grid = rf_grid
)
doParallel::registerDoParallel()
rf_tune_spec <- rand_forest(min_n = tune(), mtry = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_rec <- tree_rec
rf_tune_wflow <- workflow() %>%
add_model(rf_tune_spec) %>%
add_recipe(rf_rec)
rf_grid_search <- tune_grid(
rf_tune_wflow,
resamples = train_cv,
grid = 5
)
parallel::detectCores()
cores <- parallel::detectCores()
rf_tune_spec <- rand_forest(min_n = tune(), mtry = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
rf_rec <- tree_rec
rf_tune_wflow <- workflow() %>%
add_model(rf_tune_spec) %>%
add_recipe(rf_rec)
rf_grid_search <- tune_grid(
rf_tune_wflow,
resamples = train_cv,
grid = 5
)
parallel::stopCluster()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
train <- read_csv("./Data/train.csv") %>%
select(Cover_Type, everything())
test <- read_csv("./Data/test.csv")
set.seed(145)
train$Cover_Type <- factor(train$Cover_Type)
tree_rec <- recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "Id")
cores <- parallel::detectCores()
rf_tune_spec <- rand_forest(min_n = tune(), mtry = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
rf_rec <- tree_rec
rf_tune_wflow <- workflow() %>%
add_model(rf_tune_spec) %>%
add_recipe(rf_rec)
rf_grid_search <- tune_grid(
rf_tune_wflow,
resamples = train_cv,
grid = 5
)
rf_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(accuracy))
rf_grid_search %>%
collect_metrics()
rf_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean))
library(keras)
?keras::install_keras())
?keras::install_keras()
install_keras()
install_keras()
Y
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
train <- read_csv("./Data/train.csv") %>%
select(Cover_Type, everything())
test <- read_csv("./Data/test.csv")
set.seed(145)
svm_grid <- grid_regular(
degree(range = c(1, 10)),
cost(),
levels = 10
)
svm_tune_spec <- svm_poly(cost = tune(), degree = tune()) %>%
set_mode("classification") %>%
set_engine("kernLab")
svm_tune_spec <- svm_poly(cost = tune(), degree = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rec <- recipe() %>% recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "Id") %>%
step_pca(threshold = 0.8)
svm_grid <- grid_regular(
degree(range = c(1, 10)),
cost(),
levels = 10
)
svm_tune_spec <- svm_poly(cost = tune(), degree = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rec <- recipe() %>% recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "Id") %>%
step_pca(all_numeric(), threshold = 0.8)
svm_rec <- recipe() %>% recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "Id") %>%
step_pca(all_numeric(), threshold = 0.8, options = c(scale. = TRUE, center = TRUE))
svm_grid <- grid_regular(
degree(range = c(1, 10)),
cost(),
levels = 10
)
svm_tune_spec <- svm_poly(cost = tune(), degree = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rec <- recipe() %>% recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "id") %>%
step_pca(all_numeric(), threshold = 0.8, options = c(scale. = TRUE, center = TRUE))
svm_rec <- recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "id") %>%
step_pca(all_numeric(), threshold = 0.8, options = c(scale. = TRUE, center = TRUE))
svm_tune_wflow <- workflow() %>%
add_model(svm_tune_spec) %>%
add_recipe(svm_rec)
svm_grid_search <- tune_grid(
svm_tune_wflow,
resamples = train_cv,
grid = svm_grid
)
View(train)
glimpes(train)
glimpse(train)
train$Cover_Type <- factor(train$Cover_Type)
glimpse(train)
svm_grid <- grid_regular(
degree(range = c(1, 10)),
cost(),
levels = 10
)
svm_tune_spec <- svm_poly(cost = tune(), degree = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
svm_rec <- recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "id") %>%
step_pca(all_numeric(), threshold = 0.8, options = c(scale. = TRUE, center = TRUE))
svm_tune_wflow <- workflow() %>%
add_model(svm_tune_spec) %>%
add_recipe(svm_rec)
svm_grid_search <- tune_grid(
svm_tune_wflow,
resamples = train_cv,
grid = svm_grid
)
svm_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean))
svm_grid_search$.notes[1]
svm_rec <- recipe(Cover_Type ~ ., data=train) %>%
update_role(Id, new_role = "id") %>%
step_pca(all_numeric(), threshold = 0.8, options = c(center = TRUE))
svm_tune_wflow <- workflow() %>%
add_model(svm_tune_spec) %>%
add_recipe(svm_rec)
svm_grid_search <- tune_grid(
svm_tune_wflow,
resamples = train_cv,
grid = svm_grid
)
install.packages("xgboost")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
install.packages("xgboost")
train <- read_csv("./Data/train.csv") %>%
select(Cover_Type, everything())
set.seed(145)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), vb_train),
learn_rate(),
size = 30
)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 30
)
View(xgb_grid)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 10
)
View(xgb_grid)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 10
)
xbg_tune_spec <- boost_tree(
tree_depth = tune(),
min_n() = tune(),
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 10
)
xbg_tune_spec <- boost_tree(
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xbg_rec <- rf_rec
xbg_tune_wflow <- workflow() %>%
add_model(xbg_tune_spec) %>%
add_recipe(xbg_rec)
xbg_grid_search <- tune_grid(
xbg_tune_wflow,
resamples = train_cv,
grid = xbg_grid
)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 10
)
xgb_tune_spec <- boost_tree(
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_rec <- rf_rec
xgb_tune_wflow <- workflow() %>%
add_model(xgb_tune_spec) %>%
add_recipe(xgb_rec)
xgb_grid_search <- tune_grid(
xbg_tune_wflow,
resamples = train_cv,
grid = xgb_grid
)
xgb_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean))
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 30
)
xgb_tune_spec <- boost_tree(
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_rec <- rf_rec
xgb_tune_wflow <- workflow() %>%
add_model(xgb_tune_spec) %>%
add_recipe(xgb_rec)
xgb_grid_search <- tune_grid(
xbg_tune_wflow,
resamples = train_cv,
grid = xgb_grid
)
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 10
)
xgb_tune_spec <- boost_tree(
tree_depth = tune(),
min_n = tune(),
loss_reduction = tune(),
sample_size = tune(),
mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgb_rec <- rf_rec
xgb_tune_wflow <- workflow() %>%
add_model(xgb_tune_spec) %>%
add_recipe(xgb_rec)
xgb_grid_search <- tune_grid(
xbg_tune_wflow,
resamples = train_cv,
grid = xgb_grid
)
xgb_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean))
xgb_best_acc <- select_best(xgb_grid_search)
xgb_best_acc <- select_best(xgb_grid_search, metric = "accuracy")
xgb_spec <- finalize_model(
xgb_tune_spec,
xgb_best_acc
)
xgb_wflow <- workflow() %>%
add_model(xgb_spec) %>%
add_recipe(xgb_rec)
xgb_fit <- xgb_wflow %>%
fit(train)
View(train)
glimpse(train)
train$Cover_Type <- factor(train$Cover_Type)
xgb_fit <- xgb_wflow %>%
fit(train)
xgb_fit %>%
pull_workflow_fit() %>%
vip(geom = "col") +
theme_minimal()
library(vip)
xgb_best_acc <- select_best(xgb_grid_search, metric = "accuracy")
xgb_spec <- finalize_model(
xgb_tune_spec,
xgb_best_acc
)
xgb_wflow <- workflow() %>%
add_model(xgb_spec) %>%
add_recipe(xgb_rec)
xgb_fit <- xgb_wflow %>%
fit(train)
xgb_fit %>%
pull_workflow_fit() %>%
vip(geom = "col") +
theme_minimal()
predict(xgb_fit, new_data = test)
xgb_preds <- data.frame(Id = test$Id,
Cover_Type = predict(xgb_fit, new_data = test)$.pred_class)
xgb_preds <- data.frame(Id = test$Id,
Cover_Type = predict(xgb_fit, new_data = test)$.pred_class)
write.csv(xgb_preds, "./Predictions/xgb_preds.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
train <- read_csv("./Data/train.csv") %>% select(Cover_Type, everything())
test <- read_csv("./Data/test.csv")
set.seed(145)
# count of soil types
train %>%
select(Soil_Type1:Soil_Type40) %>%
reshape2::melt() %>%
group_by(variable) %>%
summarise(n = sum(value))
# summarize the data
glimpse(train) %>% broom::tidy()
summary(train)
summary(train) %>% broom::tidy()
summary(train) %>% knitr::kable()
# summarize the data
glimpse(train) %>% knitr::kable()
knitr::opts_chunk$set(echo = TRUE, error = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
train <- read_csv("./Data/train.csv") %>% select(Cover_Type, everything())
test <- read_csv("./Data/test.csv")
set.seed(145)
train$Cover_Type <- factor(train$Cover_Type)
# number of cores for parallel processing
cores <- parallel::detectCores()
# rf tuning specifications
rf_tune_spec <- rand_forest(min_n = tune(), mtry = tune()) %>%
set_engine("ranger", num.threads = cores) %>%
set_mode("classification")
# rf recipe (same as tree_rec)
rf_rec <- tree_rec
# rf tuning workflow
rf_tune_wflow <- workflow() %>%
add_model(rf_tune_spec) %>%
add_recipe(rf_rec)
# tune hyperparameters
rf_grid_search <- tune_grid(
rf_tune_wflow,
resamples = train_cv,
grid = 10
)
# determine best hyperparameters by accuracy
rf_grid_search %>%
collect_metrics() %>%
filter(.metric == "accuracy") %>%
arrange(desc(mean)) %>%
knitr::kable()
# summarize the data
summary(train) %>% kableExtra::
install.packages("kableExtra")
rf_preds <- data.frame(Id = test$Id,
Cover_Type = predict(rf_final_fit, new_data = test)$.pred_class)
# select best hyperparameters
rf_best_acc <- select_best(rf_grid_search, "accuracy")
# final rf specifications
rf_spec <- finalize_model(
rf_tune_spec,
rf_best_acc
) %>%
set_engine("ranger", importance = "permutation")
# final rf workflow
rf_wflow <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(rf_rec)
# final rf fit
rf_final_fit <- rf_wflow %>%
fit(train)
gc()
rf_preds <- data.frame(Id = test$Id,
Cover_Type = predict(rf_final_fit, new_data = test)$.pred_class)
write.csv(rf_preds, "./Predictions/rf_preds.csv")
knitr::opts_chunk$set(echo = TRUE, error = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(tidymodels)
train <- read_csv("./Data/train.csv") %>% select(Cover_Type, everything())
test <- read_csv("./Data/test.csv")
set.seed(145)
# summarize the data
summary(train) %>%
reshape2::melt() %>%
knitr::kable()
# grid
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train),
learn_rate(),
size = 30
)
?grid_latin_hypercube
?grid_max_entropy
remove.packages("keras")
